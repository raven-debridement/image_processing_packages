#!/usr/bin/env python

# Import required Python code.
import roslib
roslib.load_manifest('raven_pose_estimator')
import rospy

import cv
import cv2
import numpy as np
from scipy import linalg
import math
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import cv_bridge
from geometry_msgs.msg import Point, PointStamped, Quaternion, PoseStamped
import tf
import image_geometry
import time

import message_filters
from threading import Lock

import tf.transformations as tft
import tfx

import code

# adapted and adjusted from Greg Kahn's code (ImageProcessing.py)

########################################
#             CONSTANTS                #
########################################

TRANSLATION_R2L = (-0.05137, 0, 0.00136) 
ROTATION_R2L = (0, -0.108380311, 0)
#TRANSLATION_R2L = (-0.129296, 0, 0.00136124)
#ROTATION_R2L = (0,0.104719755,0)
ORANGE_LOWER = cv.Scalar( 4, 100, 100)
ORANGE_UPPER = cv.Scalar( 12, 255, 255)
BLUE_LOWER = cv.Scalar(110,100, 80)
BLUE_UPPER = cv.Scalar(120,255, 255)
SHOW_IMAGES = True
WIDTH_ROI = 250
HEIGHT_ROI = 250

########################################
#     3D GEOMETRY HELPER METHODS       #
########################################

def angleBetweenQuaternions(quat0, quat1):
    """ 
    Returns angle between quat0 and quat1 in degrees 
    """
    q0 = np.array(quat0)
    q1 = np.array(quat1)
    theta = math.acos(2*np.dot(q0,q1)**2 - 1)
    theta = theta*(180.0/math.pi)
    return theta



def order_points(p1, p2):
    """ 
    Takes in two points of form (x,y) and orders them according to y-coordinate value 
    """
    if p1[1] > p2[1]:
	return p1,p2
    elif p1[1] < p2[1]:
	return p2,p1
    else:
	return p1,p2



def find_endpoints(contours):
    """ 
    Takes in a list of contours and returns the pixel coordinates of centroids of the top two contours 
    """
    if len(contours) >=2:
	points = []
	for c in contours:
	    moments = cv2.moments(c)
	    if moments['m00']!=0:
                cx = int(moments['m10']/moments['m00'])     
                cy = int(moments['m01']/moments['m00'])        
	        points.append((cx, cy))
	upper_pt, lower_pt = order_points(points[0], points[1])
	return (lower_pt, upper_pt, True)
    else:
	return (0,0,False)



########################################
#    IMAGE-RELATED HELPER METHODS      #
########################################

def threshold(image, hsvImg, threshImg, lowerHSV, upperHSV):
    """ 
    Thresholds an image for a certain range of hsv values 
    """ 
    cv.Smooth(image, image, cv.CV_GAUSSIAN, 3, 0)
    cv.CvtColor(image, hsvImg, cv.CV_BGR2HSV) 
    cv.InRangeS(hsvImg, lowerHSV, upperHSV, threshImg)
    cv.Erode(threshImg, threshImg, None, 1)    
    cv.Dilate(threshImg, threshImg, None, 2)
    cv.Erode(threshImg, threshImg, None, 1)
    cv.Dilate(threshImg, threshImg, None, 2)
    cv.Erode(threshImg, threshImg, None, 1)
    return threshImg



def find_contours(im, name):
    """ 
    Takes in a thresholded image and returns a list of the contours 
    """
    thresh = im
    im = np.asarray(im[:,:])
    contours, hierarchy = cv2.findContours(np.asarray(thresh[:,:]),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
    return contours



##################
#   MAIN CLASS   #
##################

class ColorSegmenter():
    def __init__(self, left_camera, right_camera):
        self.leftInfo = self.rightInfo = None
        self.listener = tf.TransformListener()
	self.bridge = cv_bridge.CvBridge()

        rospy.Subscriber('/%s/image_rect_color'%left_camera, Image, self.leftImageCallback)
        rospy.Subscriber('/%s/image_rect_color'%right_camera, Image, self.rightImageCallback)
        rospy.Subscriber('/%s/camera_info'%left_camera, CameraInfo, self.leftInfoCallback)
        rospy.Subscriber('/%s/camera_info'%right_camera, CameraInfo, self.rightInfoCallback)

	self.quat_pub = rospy.Publisher('tape_orientation', Quaternion)
	self.pose_pub = rospy.Publisher('tape_pose', PoseStamped)

	self.left_orange_found = False
	self.left_blue_found = False
	self.right_orange_found = False
	self.right_blue_found = False

	self.blue_left_lower_pt = None
	self.blue_left_upper_pt = None
	self.blue_right_lower_pt = None
	self.blue_right_upper_pt = None
	self.orange_left_lower_pt = None
	self.orange_left_upper_pt = None
	self.orange_right_lower_pt = None
	self.orange_right_upper_pt = None

        self.prevQuaternion = None
    
    ################################
    #   SUBSCRIBER BOUND METHODS   #
    ################################
    def leftInfoCallback(self, info):
	""" 
        Saves the info for the left camera	
	"""
        self.leftInfo = info



    def rightInfoCallback(self, info):
	""" 
	Saves the info for the right camera	
	"""
        self.rightInfo = info



    def leftImageCallback(self, image):
	""" 	
	Takes in the image from the left camera and processes it 
	"""
	left_img, left_blue, left_orange = self.process(image, "left")
	if SHOW_IMAGES:
	    cv.ShowImage("left ROI", left_img)
	    cv.ShowImage("left blue", left_blue)
	    cv.ShowImage("left orange", left_orange)
	    cv.WaitKey(3)
	self.handleBoth()



    def rightImageCallback(self, image):
	""" 
	Takes in the image from the right camera and processes it 
	"""
	right_img, right_blue, right_orange = self.process(image, "right")
	if SHOW_IMAGES:
	    cv.ShowImage("right ROI", right_img)
	    cv.ShowImage("right blue", right_blue)
	    cv.ShowImage("right orange", right_orange)
	    cv.WaitKey(3)
	self.handleBoth()



    def process(self, image, flag):
	"""
	thresholds the image for a certain hsv range and returns the coordinates of the centroid, 
	and the coordinates of the closest point to the centroid
	"""
	cv_image = self.bridge.imgmsg_to_cv(image, "bgr8")
	ROI_left, ROI_right, pixelLeft, pixelRight = self.determineROI(flag)
	if flag == "left":
	    ROI = ROI_left
	    pixel = pixelLeft
	elif flag == "right":
	    ROI = ROI_right
            pixel = pixelRight
        heightOffset = int(pixel[1]) - HEIGHT_ROI
        widthOffset = int(pixel[0]) - WIDTH_ROI
	cv_image = cv_image[heightOffset:heightOffset+2*HEIGHT_ROI, widthOffset:widthOffset+2*WIDTH_ROI]
	hsvImg = cv.CreateImage(cv.GetSize(cv_image),8,3)
	orangeThreshImg = cv.CreateImage(cv.GetSize(cv_image),8,1)
	orangeThreshImg = threshold(cv_image, hsvImg, orangeThreshImg, ORANGE_LOWER, ORANGE_UPPER)
	blueThreshImg = cv.CreateImage(cv.GetSize(cv_image),8,1)
	blueThreshImg = threshold(cv_image, hsvImg, blueThreshImg, BLUE_LOWER, BLUE_UPPER)
	blue_contours = find_contours(blueThreshImg, "blue") #FIXME remove later
	orange_contours = find_contours(orangeThreshImg, "orange") #FIXME remove later
	if flag=="left":
	    self.blue_left_lower_pt, self.blue_left_upper_pt, self.left_blue_found = find_endpoints(blue_contours)
	    self.orange_left_lower_pt, self.orange_left_upper_pt, self.left_orange_found = find_endpoints(orange_contours)
            self.blue_left_lower_pt = self.addOffset(self.blue_left_lower_pt, widthOffset, heightOffset)
            self.blue_left_upper_pt = self.addOffset(self.blue_left_upper_pt, widthOffset, heightOffset)
            self.orange_left_lower_pt = self.addOffset(self.orange_left_lower_pt, widthOffset, heightOffset)
            self.orange_left_upper_pt = self.addOffset(self.orange_left_upper_pt, widthOffset, heightOffset)
        elif flag=="right":
	    self.blue_right_lower_pt, self.blue_right_upper_pt, self.right_blue_found = find_endpoints(blue_contours)
	    self.orange_right_lower_pt, self.orange_right_upper_pt, self.right_orange_found = find_endpoints(orange_contours)
            self.blue_right_lower_pt = self.addOffset(self.blue_right_lower_pt, widthOffset, heightOffset)
            self.blue_right_upper_pt = self.addOffset(self.blue_right_upper_pt, widthOffset, heightOffset)
            self.orange_right_lower_pt = self.addOffset(self.orange_right_lower_pt, widthOffset, heightOffset)
            self.orange_right_upper_pt = self.addOffset(self.orange_right_upper_pt, widthOffset, heightOffset)
        return cv_image, blueThreshImg, orangeThreshImg



    def handleBoth(self):
	""" 
	Returns the quaternion and position of the pieces of tape (which should correspond to the 
	orientation and position of the gripper 
	"""
	found = self.left_orange_found and self.left_blue_found and self.right_orange_found and self.left_orange_found
	if found:
	    blue_lower_pt = self.convertStereo(self.blue_left_lower_pt[0], self.blue_left_lower_pt[1], math.fabs(self.blue_left_lower_pt[0] - self.blue_right_lower_pt[0]))
	    blue_upper_pt = self.convertStereo(self.blue_left_upper_pt[0], self.blue_left_upper_pt[1], math.fabs(self.blue_left_upper_pt[0] - self.blue_right_upper_pt[0]))
	    orange_lower_pt = self.convertStereo(self.orange_left_lower_pt[0], self.orange_left_lower_pt[1], math.fabs(self.orange_left_lower_pt[0] - self.orange_right_lower_pt[0]))
	    orange_upper_pt = self.convertStereo(self.orange_left_upper_pt[0], self.orange_left_lower_pt[1], math.fabs(self.orange_left_upper_pt[0] - self.orange_right_upper_pt[0]))
	    p = Point()
	    position = ((blue_lower_pt[0]+blue_upper_pt[0]+orange_lower_pt[0]+orange_upper_pt[0])/4, (blue_lower_pt[1]+blue_upper_pt[1]+orange_lower_pt[1]+orange_upper_pt[1])/4, (blue_lower_pt[2]+blue_upper_pt[2]+orange_lower_pt[2]+orange_upper_pt[2])/4)
	    p.x = position[0]
	    p.y = position[1]
	    p.z = position[2]

	    blue_vector = []
	    orange_vector = []
	    for i in range(0, 3):
		blue_vector.append(blue_upper_pt[i] - blue_lower_pt[i])
		orange_vector.append(orange_upper_pt[i] - orange_lower_pt[i])
	    quat = self.get_orientation_from_lines(blue_vector, orange_vector)
	    tb = tfx.tb_angles(quat)
	    q = Quaternion()
	    q.x = quat[0]
	    q.y = quat[1]
	    q.z = quat[2]
	    q.w = quat[3]

            if self.prevQuaternion == None:
                self.prevQuaternion = q
            else:
                prevQuat = tfx.tb_angles(self.prevQuaternion).msg
                currQuat = tfx.tb_angles(q).msg
                angle = self.angleBetweenQuaternions(currQuat, prevQuat)
                q = prevQuat = tfx.tb_angles(currQuat).msg
	    pose = PoseStamped()
	    pose.pose.position = p
	    pose.pose.orientation = q
	    pose.header.frame_id = self.leftInfo.header.frame_id
	    pose.header.stamp = rospy.Time.now()
	    self.quat_pub.publish(q)
	    self.pose_pub.publish(pose)



    ##############################
    #       HELPER METHODS       #
    ############################## 

    def determineROI(self, flag):
	"""
	Determines the boundaries of a region of interest, based on 
	the pixel coordinates of the gripper as given by the inverse kinematics of the robot
	"""
	pose = tfx.pose([0,0,0], [0,0,0,1], frame='/tool_L', stamp=rospy.Time.now())
	tf_tool_to_cam = tfx.lookupTransform('/left_optical_frame','/tool_L',wait=10)	
	pose = tf_tool_to_cam * pose
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
	(u_l, v_l), (u_r, v_r) = stereoModel.project3dToPixel(pose.position.list)
	ROI_left = ((int(u_l-WIDTH_ROI), int(v_l-HEIGHT_ROI)), (int(u_l+WIDTH_ROI), int(v_l+HEIGHT_ROI)))
	ROI_right= ((int(u_r-WIDTH_ROI), int(v_r-HEIGHT_ROI)), (int(u_r+WIDTH_ROI), int(v_r+HEIGHT_ROI)))
	return ROI_left, ROI_right, (u_l, v_l), (u_r, v_r)
	

    def addOffset(self, pt, xOffset, yOffset):
	"""
	Adds x and y offsets to a point of form (x, y)
	"""
        if type(pt) == tuple:
            return (pt[0]+xOffset,pt[1]+yOffset)
        return pt


    def get_orientation_from_lines(self, v0, v1):
        """ 
	Takes in two vectors, v0 and v1, (which are KNOWN to be in the same plane) and finds 
	the normal, and creates a rotation matrix from v0, v1, and the normal; then converts this 
	rotation matrix to a quaternion 
	"""
        v0, v1 = np.array(v0), np.array(v1)
        v0 = v0 / np.linalg.norm(v0) 
        v1 = v1 / np.linalg.norm(v1) 
        n = np.cross(v0, v1)
        n = n / np.linalg.norm(n)
        v1 = np.cross(n, v0)
        v1 = v1 / np.linalg.norm(v1)
	rotMat = np.vstack((n, v1, v0)).T
	matrix = rotMat
        tbRot = tfx.tb_angles(matrix).matrix	
	tbRot = self.rotate(45, "yaw", tbRot)    #FIXME: get correct yaw pitch roll values
	tbRot = self.rotate(60, "pitch", tbRot)
	tbRot = self.rotate(45, "roll", tbRot)
        quat = tfx.tb_angles(tbRot).quaternion
        return list(quat)


    def rotate(self, angle, axis, matrix):
	""" 
	Takes a rotation matrix and rotates it a certain angle about a certain axis 
	"""
	if axis == "yaw":
	    rot = tfx.tb_angles(angle, 0, 0).matrix
	elif axis == "pitch":
	    rot = tfx.tb_angles(0, angle, 0).matrix
	elif axis == "roll":
	    rot = tfx.tb_angles(0, 0, angle).matrix
	return rot*matrix


    def convertStereo(self, u, v, disparity):
        """ 
	Converts two pixel coordinates u and v along with the disparity to give PointStamped 
	"""
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
        (x,y,z) = stereoModel.projectPixelTo3d((u,v), disparity)
        return (x,y,z)


    def angleBetweenQuaternions(self, quat0, quat1):
        """ 
	Returns angle between quat0 and quat1 in degrees 
	"""
        q0 = np.array([quat0.x, quat0.y, quat0.z, quat0.w])
        q1 = np.array([quat1.x, quat1.y, quat1.z, quat1.w])
        try:
            theta = math.acos(2*np.dot(q0,q1)**2 - 1)
        except ValueError:
            return 0
        theta = theta*(180.0/math.pi)
        return theta


##############################
#      EXECUTION CODE        #
##############################

def main():
    rospy.init_node('color_segmenter',anonymous=True)
    rospy.sleep(5)
    left_camera = 'left'
    right_camera = 'right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()


def test_cache():
    rospy.init_node('test_cache',anonymous=True)
    rospy.sleep(1)
    listener = tf.TransformListener()
    f0 = '/left_optical_frame'
    f1 = '/0_link'
    while not rospy.is_shutdown():
        try:
            common = listener.getLatestCommonTime(f0, f1)
            listener.waitForTransform(f0,f1,common,rospy.Duration(4.0))
            break
        except tf.Exception:
            rospy.loginfo('tf exception')

        rospy.sleep(.1)
        
    rospy.loginfo('found it!')

if __name__ == '__main__':
    main()
    

