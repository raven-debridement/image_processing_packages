#!/usr/bin/env python

# Import required Python code.
import roslib
roslib.load_manifest('raven_pose_estimator')
import rospy

from raven_pose_estimator.srv import *
import cv
import cv2
import numpy as np
from scipy import linalg
import math
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import cv_bridge
from geometry_msgs.msg import Point, PointStamped, Quaternion, PoseStamped, Polygon, PolygonStamped, Point32
import tf
import image_geometry
import time

import message_filters
from threading import Lock

import tf.transformations as tft
import tfx

import code

# adapted and adjusted from Greg Kahn's code (ImageProcessing.py)

########################################
#             CONSTANTS                #
########################################

#TRANSLATION_R2L = (-0.05137, 0, 0.00136) # narrow baseline
#ROTATION_R2L = (0, -0.108380311, 0)
#TRANSLATION_R2L = (-0.051374, 0, 0.00136) # wide baseline 1
#TRANSLATION_R2L = (-0.0773479, 0, 0.00136) # wide baseline 2
TRANSLATION_R2L = (-0.103322, 0, 0.00136) # wide baseline 3
#TRANSLATION_R2L = (-0.129296, 0, 0.00136124) # wide baseline 4
ROTATION_R2L = (0,0.104719755,0) # wide baseline rotation; same for all sets

#ORANGE_LOWER = cv.Scalar( 6, 125, 130) # THESE ARE THE ORANGE VALUES
#ORANGE_UPPER = cv.Scalar( 20, 255, 255)
#ORANGE_LOWER = cv.Scalar(90, 150, 30) # THESE ARE THE GREEN VALUES
#ORANGE_UPPER = cv.Scalar(96, 255, 255)
#BLUE_LOWER = cv.Scalar(28,30, 100) # THESE ARE THE YELLOW VALUES
#BLUE_UPPER = cv.Scalar(42,255, 255)
#BLUE_LOWER = cv.Scalar(112, 150, 50) # THESE ARE THE DARK BLUE VALUES
#BLUE_UPPER = cv.Scalar(118, 255, 255)
#BLUE_LOWER = cv.Scalar(105, 180, 100) # THESE ARE THE MASKING TAPE BLUE VALUES pretty good
#BLUE_UPPER = cv.Scalar(115, 255, 255)
#BLUE_LOWER = cv.Scalar(90, 100, 100) # THESE ARE THE BLUE PAPER VALUES
#BLUE_UPPER = cv.Scalar(108, 255, 255)
BLUE_LOWER = cv.Scalar(80, 50, 50) #THESE ARE THE GREEN PAPER VALUES
BLUE_UPPER = cv.Scalar(100, 255, 255)
#ORANGE_LOWER = cv.Scalar(80, 80, 100) # THESE ARE THE MASKING TAPE GREEN VALUES ehh
#ORANGE_UPPER = cv.Scalar(95, 150, 255) 
#ORANGE_LOWER = cv.Scalar(113, 90, 100) # THESE ARE THE PURPLE PAPER VALUES
#ORANGE_UPPER = cv.Scalar(120, 200, 255)
#ORANGE_LOWER = cv.Scalar(148, 20, 100) # THESE ARE THE PINK TAPE VALUES
#ORANGE_UPPER = cv.Scalar(170, 120, 255)
ORANGE_LOWER = cv.Scalar(4, 40, 100) # THESE ARE THE ORANGE PAPER VALUES
ORANGE_UPPER = cv.Scalar(30, 255, 255)

RED_LOWER = cv.Scalar(0, 100, 30) # VALUES FOR THE RED FOAM
RED_UPPER = cv.Scalar(3, 255, 255)
SHOW_IMAGES = False
WIDTH_ROI = 250
HEIGHT_ROI = 250
SLERP = True
SLERP_CONSTANT = 0.2
PRINT_MESSAGES = False
SHOW_TIME = True



########################################
#     3D GEOMETRY HELPER METHODS       #
########################################

def angleBetweenQuaternions(quat0, quat1):
    """ 
    Returns angle between quat0 and quat1 in degrees 
    """
    q0 = np.array(quat0)
    q1 = np.array(quat1)
    theta = math.acos(2*np.dot(q0,q1)**2 - 1)
    theta = theta*(180.0/math.pi)
    return theta



def order_points(p1, p2):
    """ 
    Takes in two points of form (x,y) and orders them according to y-coordinate value 
    """
    if p1[1] > p2[1]:
	return p1,p2
    elif p1[1] < p2[1]:
	return p2,p1
    else:
	return p1,p2



def find_endpoints(contours):
    """ 
    Takes in a list of contours and returns the pixel coordinates of centroids of the top two contours 
    """
    if len(contours) < 2:
        return (0,0,False)
    contours.sort(key=len, reverse=True)
    points = []
    for c in contours:
        moments = cv2.moments(c)
        if moments['m00']!=0:
            cx = int(moments['m10']/moments['m00'])     
            cy = int(moments['m01']/moments['m00'])        
            points.append((cx, cy))
    try:
        upper_pt, lower_pt = order_points(points[0], points[1])
        return (lower_pt, upper_pt, True)
    except IndexError as e:
        return (0,0,False)	
    else:
        return (0,0,False)


def quat_dot_product(q1, q2):
    """
    takes in two quaternions and returns their dot product
    """
    return q1.x*q2.x + q1.y*q2.y + q1.z*q2.z +q1.w*q2.w


def slerp(q1, q2, t):
    """
    finds the SLERP interpolation between two quaternions
    """
    dot_product = quat_dot_product(q1,q2)
    if dot_product < 0:
	q1.w = -q1.w
	dot_product = quat_dot_product(q1, q2)
    theta = math.acos(dot_product)
    x = (math.sin((1-t)*theta)/math.sin(theta))*q1.x + (math.sin(t*theta)/math.sin(theta))*q2.x
    y = (math.sin((1-t)*theta)/math.sin(theta))*q1.y + (math.sin(t*theta)/math.sin(theta))*q2.y
    z = (math.sin((1-t)*theta)/math.sin(theta))*q1.z + (math.sin(t*theta)/math.sin(theta))*q2.z
    w = (math.sin((1-t)*theta)/math.sin(theta))*q1.w + (math.sin(t*theta)/math.sin(theta))*q2.w
    result = Quaternion()
    result.x = x
    result.y = y
    result.z = z
    result.w = w
    return result


########################################
#    IMAGE-RELATED HELPER METHODS      #
########################################

def threshold(image, hsvImg, threshImg, lowerHSV, upperHSV):
    """ 
    Thresholds an image for a certain range of hsv values 
    """ 
    cv.Smooth(image, image, cv.CV_GAUSSIAN, 3, 0)
    cv.CvtColor(image, hsvImg, cv.CV_BGR2HSV) 
    cv.InRangeS(hsvImg, lowerHSV, upperHSV, threshImg)
    cv.Erode(threshImg, threshImg, None, 1)
    cv.Dilate(threshImg, threshImg, None, 1)
    #cv.Erode(threshImg, threshImg, None, 2)    
    #cv.Dilate(threshImg, threshImg, None, 1)
    cv.Erode(threshImg, threshImg, None, 1)
    cv.Dilate(threshImg, threshImg, None, 1)
    #cv.Erode(threshImg, threshImg, None, 1)
    return threshImg

def average_vectors(v1, v2):
    """
    Finds the average of two vectors v1 and v2
    """
    result = []
    for i in range(0, len(v1)):
	result.append((v1[i]+v2[i])/2)
    return result

def find_contours(im, name):
    """ 
    Takes in a thresholded image and returns a list of the contours 
    """
    thresh = im
    im = np.asarray(im[:,:])
    contours, hierarchy = cv2.findContours(np.asarray(thresh[:,:]),cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)
    return contours



##################
#   MAIN CLASS   #
##################

class ColorSegmenter():
    def __init__(self, left_camera, right_camera):
        self.leftInfo = self.rightInfo = None
        self.listener = tf.TransformListener()
	self.bridge = cv_bridge.CvBridge()
	self.leftImg = self.rightImg = None

        rospy.Subscriber('/%s/image_rect_color'%left_camera, Image, self.leftImageCallback)
        rospy.Subscriber('/%s/image_rect_color'%right_camera, Image, self.rightImageCallback)
        rospy.Subscriber('/%s/camera_info'%left_camera, CameraInfo, self.leftInfoCallback)
        rospy.Subscriber('/%s/camera_info'%right_camera, CameraInfo, self.rightInfoCallback)

	self.quat_pub = rospy.Publisher('tape_orientation', Quaternion)
	self.pose_pub = rospy.Publisher('tape_pose', PoseStamped)
	self.polygon_pub = rospy.Publisher('polygon', PolygonStamped)
        
 	self.red_service = rospy.Service('thresh_red', ThreshRed, self.threshRed)

	self.left_orange_found = False
	self.left_blue_found = False
	self.right_orange_found = False
	self.right_blue_found = False

	self.blue_left_lower_pt = None
	self.blue_left_upper_pt = None
	self.blue_right_lower_pt = None
	self.blue_right_upper_pt = None
	self.orange_left_lower_pt = None
	self.orange_left_upper_pt = None
	self.orange_right_lower_pt = None
	self.orange_right_upper_pt = None

        self.prevQuaternion = None
    
    ################################
    #   SUBSCRIBER BOUND METHODS   #
    ################################
    def leftInfoCallback(self, info):
	""" 
        Saves the info for the left camera	
	"""
        self.leftInfo = info


    def rightInfoCallback(self, info):
	""" 
	Saves the info for the right camera	
	"""
        self.rightInfo = info
	

    def leftImageCallback(self, image):
	""" 	
	Takes in the image from the left camera and processes it 
	"""
	self.t0_left = time.clock()
	left_img, left_blue, left_orange, left_blue_contour, left_orange_contour, self.left_blue_found, self.left_orange_found = self.process(image, "left")
	if SHOW_IMAGES:
	    cv.ShowImage("left ROI", left_img)
	    cv.ShowImage("left blue", left_blue)
	    cv.ShowImage("left orange", left_orange)
        cv.WaitKey(3)
	self.handleBoth()
	self.left_blue_found = self.left_orange_found = False
	

    def rightImageCallback(self, image):
	""" 
	Takes in the image from the right camera and processes it 
	"""
	self.t0_right = time.clock()
	right_img, right_blue, right_orange, right_blue_contour, right_orange_contour, self.right_blue_found, self.right_orange_found = self.process(image, "right")
	if SHOW_IMAGES:
	    cv.ShowImage("right ROI", right_img)
	    cv.ShowImage("right blue", right_blue)
	    cv.ShowImage("right orange", right_orange)
        cv.WaitKey(3)
	self.handleBoth()
	self.right_blue_found = self.right_orange_found = False


    def threshRed(self, i):
	#cv.ShowImage("self.leftImg", self.leftImg)
	leftImg = cv.CloneMat(self.leftImg)
	rightImg = cv.CloneMat(self.rightImg)
	width = cv.GetSize(leftImg)[0]
	height = cv.GetSize(leftImg)[1]
	leftImg = leftImg[0:200, :]
	rightImg = rightImg[0:200, :]
	if SHOW_IMAGES:
	    cv.ShowImage("smaller left", leftImg)
	    cv.ShowImage("smaller right", rightImg)
	leftThresh = cv.CreateImage(cv.GetSize(leftImg),8,1)
	rightThresh = cv.CreateImage(cv.GetSize(rightImg),8,1)	
	leftThresh = threshold(leftImg, leftImg, leftThresh, RED_LOWER, RED_UPPER)
	rightThresh = threshold(rightImg, rightImg, rightThresh, RED_LOWER, RED_UPPER)
	if SHOW_IMAGES:
	    cv.ShowImage("threshleft", leftThresh)
	    cv.ShowImage("threshRight", rightThresh)
        cv.WaitKey(3)
	left_contours = find_contours(leftThresh, "left_thresh")
	right_contours = find_contours(rightThresh, "right_thresh")
	if len(left_contours) > 0 and len(right_contours)>0:
	    rospy.loginfo(len(left_contours))
	    rospy.loginfo(len(right_contours))
	    return ThreshRedResponse(1)
	return ThreshRedResponse(0)


    def process(self, image, flag):
	"""
	thresholds the image for a certain hsv range and returns the coordinates of the centroid, 
	and the coordinates of the closest point to the centroid
	"""
	cv_image = self.bridge.imgmsg_to_cv(image, "bgr8")
	ROI_left, ROI_right, pixelLeft, pixelRight = self.determineROI(flag)
	if flag == "left":
	    ROI = ROI_left
	    pixel = pixelLeft
	elif flag == "right":
	    ROI = ROI_right
            pixel = pixelRight
        heightOffset = int(pixel[1]) - HEIGHT_ROI
        widthOffset = int(pixel[0]) - WIDTH_ROI
	if heightOffset < 0:
	    heightOffset = 0
	if widthOffset < 0:
	    widthOffset = 0
	cv_image = cv_image[heightOffset:heightOffset+2*HEIGHT_ROI, widthOffset:widthOffset+2*WIDTH_ROI]
	if flag == "left":
	    self.leftImg = cv_image
	elif flag == "right":
	    self.rightImg = cv_image
	hsvImg = cv.CreateImage(cv.GetSize(cv_image),8,3)
	orangeThreshImg = cv.CreateImage(cv.GetSize(cv_image),8,1)
	orangeThreshImg = threshold(cv_image, hsvImg, orangeThreshImg, ORANGE_LOWER, ORANGE_UPPER)
	blueThreshImg = cv.CreateImage(cv.GetSize(cv_image),8,1)
	blueThreshImg = threshold(cv_image, hsvImg, blueThreshImg, BLUE_LOWER, BLUE_UPPER)
	blueContourImg = cv.CloneImage(blueThreshImg)
	orangeContourImg = cv.CloneImage(orangeThreshImg)
	blue_contours = find_contours(blueContourImg, "blue") #FIXME remove later
	orange_contours = find_contours(orangeContourImg, "orange") #FIXME remove later
	if flag=="left":
	    self.blue_left_lower_pt, self.blue_left_upper_pt, left_blue_found = find_endpoints(blue_contours)
	    self.orange_left_lower_pt, self.orange_left_upper_pt, left_orange_found = find_endpoints(orange_contours)
            self.blue_left_lower_pt = self.addOffset(self.blue_left_lower_pt, widthOffset, heightOffset)
            self.blue_left_upper_pt = self.addOffset(self.blue_left_upper_pt, widthOffset, heightOffset)
            self.orange_left_lower_pt = self.addOffset(self.orange_left_lower_pt, widthOffset, heightOffset)
            self.orange_left_upper_pt = self.addOffset(self.orange_left_upper_pt, widthOffset, heightOffset)
	    blue_found = left_blue_found
	    orange_found = left_orange_found
        elif flag=="right":
	    self.blue_right_lower_pt, self.blue_right_upper_pt, right_blue_found = find_endpoints(blue_contours)
	    self.orange_right_lower_pt, self.orange_right_upper_pt, right_orange_found = find_endpoints(orange_contours)
            self.blue_right_lower_pt = self.addOffset(self.blue_right_lower_pt, widthOffset, heightOffset)
            self.blue_right_upper_pt = self.addOffset(self.blue_right_upper_pt, widthOffset, heightOffset)
            self.orange_right_lower_pt = self.addOffset(self.orange_right_lower_pt, widthOffset, heightOffset)
            self.orange_right_upper_pt = self.addOffset(self.orange_right_upper_pt, widthOffset, heightOffset)
	    blue_found = right_blue_found
	    orange_found = right_orange_found
        return cv_image, blueThreshImg, orangeThreshImg, blueContourImg, orangeContourImg, blue_found, orange_found


    def handleBoth(self):
	""" 
	Returns the quaternion and position of the pieces of tape (which should correspond to the 
	orientation and position of the gripper 
	"""
	try:
	    found = self.left_orange_found and self.left_blue_found and self.right_orange_found and self.right_blue_found
	    if PRINT_MESSAGES:
	        print "found"
	        print "left_orange: "+str(self.left_orange_found)
	        print "right_orange: "+str(self.right_orange_found)
	        print "left_blue: "+str(self.left_blue_found)
	        print "right_blue: "+str(self.right_blue_found)
	    if found:
	        blue_lower_pt = self.convertStereo(self.blue_left_lower_pt[0], self.blue_left_lower_pt[1], math.fabs(self.blue_left_lower_pt[0] - self.blue_right_lower_pt[0]))
	        blue_upper_pt = self.convertStereo(self.blue_left_upper_pt[0], self.blue_left_upper_pt[1], math.fabs(self.blue_left_upper_pt[0] - self.blue_right_upper_pt[0]))
	        orange_lower_pt = self.convertStereo(self.orange_left_lower_pt[0], self.orange_left_lower_pt[1], math.fabs(self.orange_left_lower_pt[0] - self.orange_right_lower_pt[0]))
	        orange_upper_pt = self.convertStereo(self.orange_left_upper_pt[0], self.orange_left_upper_pt[1], math.fabs(self.orange_left_upper_pt[0] - self.orange_right_upper_pt[0]))
	        p = Point()
	        position = ((blue_lower_pt[0]+blue_upper_pt[0]+orange_lower_pt[0]+orange_upper_pt[0])/4, (blue_lower_pt[1]+blue_upper_pt[1]+orange_lower_pt[1]+orange_upper_pt[1])/4, (blue_lower_pt[2]+blue_upper_pt[2]+orange_lower_pt[2]+orange_upper_pt[2])/4)
	        p.x = position[0]
	        p.y = position[1]
	        p.z = position[2]

	        polygon_points = [blue_lower_pt, blue_upper_pt, orange_upper_pt, orange_lower_pt]
	        polygon_points = [Point32(*x) for x in polygon_points]
	        polygon = PolygonStamped()
	        polygon.polygon.points = polygon_points
	        polygon.header.frame_id = "left_BC"
	        polygon.header.stamp = rospy.Time.now()
	        self.polygon_pub.publish(polygon)
	        blue_vector = []
	        orange_vector = []
	        for i in range(0, 3):
		    blue_vector.append(blue_upper_pt[i] - blue_lower_pt[i])
		    orange_vector.append(orange_upper_pt[i] - orange_lower_pt[i])
	        quat = self.get_orientation_from_lines(blue_vector, orange_vector)
	        tb = tfx.tb_angles(quat)
	        q = Quaternion()
	        q.x = quat[0]
	        q.y = quat[1]
	        q.z = quat[2]
	        q.w = quat[3]

                if self.prevQuaternion == None:
                    self.prevQuaternion = q
                else:
                    prevQuat = tfx.tb_angles(self.prevQuaternion).msg
                    currQuat = tfx.tb_angles(q).msg
	            if SLERP:
			try:
		            q = slerp(prevQuat, currQuat, SLERP_CONSTANT)
			except ZeroDivisionError:
			    q = currQuat
		        self.prevQuaternion = q
	            else:
		        q = currQuat
	 	        self.prevQuaternion = q
	        pose = PoseStamped()
	        pose.pose.position = p
	        pose.pose.orientation = q
	        pose.header.frame_id = self.leftInfo.header.frame_id
	        pose.header.stamp = rospy.Time.now()
	        self.quat_pub.publish(q)
	        self.pose_pub.publish(pose)
		t_left = time.clock() - self.t0_left
		t_right = time.clock() - self.t0_right
		if SHOW_TIME:
		    print "RUNNING TIME FROM LEFT IMAGE: "+str(t_left)
		    print "RUNNING TIME FROM RIGHT IMAGE: "+str(t_right)
		if PRINT_MESSAGES:		
	            print "WORKING FINE"
	    else:
		#self.pose_pub.publish(self.prevQuaternion) #if the points aren't detected, continue publishing the last known location
		if PRINT_MESSAGES:		
		     print "POINTS NOT DETECTED"
	except (TypeError, ValueError) as e:
	    if PRINT_MESSAGES:
   	        print "CAUGHT ERROR"	
	        print e
	    #self.pose_pub.publish(self.prevQuaternion) #if some kind of error comes up, continue publishing the last known location #FIXME we may want to change this behavior
	    pass



    ##############################
    #       HELPER METHODS       #
    ############################## 

    ########################################
    #       RED THRESHOLD SERVER           #
    ########################################


    def determineROI(self, flag):
	"""
	Determines the boundaries of a region of interest, based on 
	the pixel coordinates of the gripper as given by the inverse kinematics of the robot
	"""
	pose = tfx.pose([0,0,0], [0,0,0,1], frame='/tool_L', stamp=rospy.Time.now())
	tf_tool_to_cam = tfx.lookupTransform('/left_BC','/tool_L',wait=10)	
	pose = tf_tool_to_cam * pose
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
	(u_l, v_l), (u_r, v_r) = stereoModel.project3dToPixel(pose.position.list)
	ROI_left = ((int(u_l-WIDTH_ROI), int(v_l-HEIGHT_ROI)), (int(u_l+WIDTH_ROI), int(v_l+HEIGHT_ROI)))
	ROI_right= ((int(u_r-WIDTH_ROI), int(v_r-HEIGHT_ROI)), (int(u_r+WIDTH_ROI), int(v_r+HEIGHT_ROI)))
	return ROI_left, ROI_right, (u_l, v_l), (u_r, v_r)
	

    def addOffset(self, pt, xOffset, yOffset):
	"""
	Adds x and y offsets to a point of form (x, y)
	"""
        if type(pt) == tuple:
            return (pt[0]+xOffset,pt[1]+yOffset)
        return pt


    def get_orientation_from_lines(self, v0, v1):
        """ 
	Takes in two vectors, v0 and v1, (which are KNOWN to be in the same plane) and finds 
	the normal, and creates a rotation matrix from v0, v1, and the normal; then converts this 
	rotation matrix to a quaternion 
	"""
        v0, v1 = np.array(v0), np.array(v1)
        v0 = v0 / np.linalg.norm(v0) 
        v1 = v1 / np.linalg.norm(v1) 
        n = np.cross(v0, v1)
	parallel = average_vectors(v0, v1)
	parallel = parallel / np.linalg.norm(parallel)
	third = np.cross(n, parallel)
	third = third / np.linalg.norm(third)
        #n = n / np.linalg.norm(n)
        #v1 = np.cross(n, v0)
        #v1 = v1 / np.linalg.norm(v1)
	#rotMat = np.vstack((n, v1, v0)).T
	rotMat = np.vstack((parallel, third, n)).T
	matrix = rotMat
        tbRot = tfx.tb_angles(matrix).matrix	
	#tbRot = self.rotate(-90, "yaw", tbRot)    #FIXME: get correct yaw pitch roll values
	#tbRot = self.rotate(60, "pitch", tbRot)
	tbRot = self.rotate(180, "roll", tbRot)
        quat = tfx.tb_angles(tbRot).quaternion
        return list(quat)


    def rotate(self, angle, axis, matrix):
	""" 
	Takes a rotation matrix and rotates it a certain angle about a certain axis 
	"""
	if axis == "yaw":
	    rot = tfx.tb_angles(angle, 0, 0).matrix
	elif axis == "pitch":
	    rot = tfx.tb_angles(0, angle, 0).matrix
	elif axis == "roll":
	    rot = tfx.tb_angles(0, 0, angle).matrix
	return matrix*rot


    def convertStereo(self, u, v, disparity):
        """ 
	Converts two pixel coordinates u and v along with the disparity to give PointStamped 
	"""
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
        (x,y,z) = stereoModel.projectPixelTo3d((u,v), disparity)
        return (x,y,z)


##############################
#      EXECUTION CODE        #
##############################

def main():
    rospy.init_node('color_segmenter',anonymous=True)
    rospy.sleep(5)
    left_camera = '/BC/left'
    right_camera = '/BC/right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()


def test_cache():
    rospy.init_node('test_cache',anonymous=True)
    rospy.sleep(1)
    listener = tf.TransformListener()
    f0 = '/left_optical_frame'
    f1 = '/0_link'
    while not rospy.is_shutdown():
        try:
            common = listener.getLatestCommonTime(f0, f1)
            listener.waitForTransform(f0,f1,common,rospy.Duration(4.0))
            break
        except tf.Exception:
            rospy.loginfo('tf exception')

        rospy.sleep(.1)
        
    rospy.loginfo('found it!')

if __name__ == '__main__':
    main()
    

