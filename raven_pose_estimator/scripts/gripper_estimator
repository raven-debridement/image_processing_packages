#!/usr/bin/env python

# Import required Python code.
import roslib
roslib.load_manifest('raven_pose_estimator')
import rospy

import cv
import cv2
import numpy as np
import math
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import cv_bridge
from geometry_msgs.msg import Point, PointStamped
import tf
import image_geometry

import message_filters
from threading import Lock

# adapted and adjusted from Greg Kahn's code (ImageProcessing.py)

class GripperSegmenter():
    def __init__(self, left_camera, right_camera):
        
        self.leftInfo = self.rightInfo = None

        self.foundColorLeft = False
        self.foundColorRight = False
	
        self.listener = tf.TransformListener()
	self.bridge = cv_bridge.CvBridge()
        self.outputFrame = 'base_link'
        rospy.Subscriber('%s/image_rect_color'%left_camera, Image, self.leftImageCallback)
        rospy.Subscriber('%s/image_rect_color'%right_camera, Image, self.rightImageCallback)
        rospy.Subscriber('%s/camera_info'%left_camera, CameraInfo, self.leftInfoCallback)
        rospy.Subscriber('%s/camera_info'%right_camera, CameraInfo, self.rightInfoCallback)

    def convertStereo(self, u, v, disparity):
        """
        Converts two pixel coordinates u and v along with the disparity to give PointStamped       
        """
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
        (x,y,z) = stereoModel.projectPixelTo3d((u,v), disparity)
        
        cameraPoint = PointStamped()
        cameraPoint.header.frame_id = self.leftInfo.header.frame_id
        cameraPoint.header.stamp = rospy.Time.now()
        cameraPoint.point = Point(x,y,z)

        self.listener.waitForTransform(self.outputFrame, cameraPoint.header.frame_id, rospy.Time.now(), rospy.Duration(4.0))
        outputPoint = self.listener.transformPoint(self.outputFrame, cameraPoint)
        return outputPoint
    
    def leftInfoCallback(self, info):
        self.leftInfo = info

    def rightInfoCallback(self, info):
        self.rightInfo = info

    def leftImageCallback(self, image):
	left_thresh = self.threshold(image, "left")
        cv.ShowImage('Left Viewer', left_thresh)
        cv.WaitKey(3)

    def rightImageCallback(self, image):
	right_thresh = self.threshold(image, "right")
        cv.ShowImage('Right Viewer', right_thresh)
        cv.WaitKey(3)


    def threshold(self, image, flag):
	cv_image = self.bridge.imgmsg_to_cv(image, "bgr8")
	gray_image = cv2.cvtColor(np.asarray(cv_image[:,:]), cv2.COLOR_BGR2GRAY)
	#(thresh, im_bw) = cv2.threshold(gray_image, 200, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
	edges = cv2.Canny(gray_image, 80, 120)
	lines = cv2.HoughLinesP(edges, 1, math.pi/180, 1, None, 50, 1)
	#print lines
	print flag
	#print len(lines)
	for line in lines[0]:
	    print line
	return cv.fromarray(edges) #cv.fromarray(im_bw)
		
        

#takes in an image and returns the centroid of its blob
def findCentroid(image):
	mat = cv.GetMat(image)
	yxCoords = []
	for x in range(mat.width):
	    for y in range(mat.height):
	        if mat[y,x] > 0.0:
	            yxCoords.append((y,x))
	if len(yxCoords) == 0:
	    return                     
	yCentroid = sum([y for y,x in yxCoords])/len(yxCoords)
	xCentroid = sum([x for y,x in yxCoords])/len(yxCoords)
	return (xCentroid, yCentroid)




def main():
    rospy.init_node('gripper_estimator')
    left_camera = 'left'
    right_camera = 'right'
    gs = GripperSegmenter(left_camera, right_camera)
    rospy.spin()


if __name__ == '__main__':
    main()
    #test()
    #webcam()
