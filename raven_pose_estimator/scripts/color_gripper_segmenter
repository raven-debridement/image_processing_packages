#!/usr/bin/env python

# Import required Python code.
import roslib
roslib.load_manifest('raven_pose_estimator')
import rospy

import cv
import cv2
import numpy as np
import math
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import cv_bridge
from geometry_msgs.msg import Point, PointStamped
import tf
import image_geometry
import time

import message_filters
from threading import Lock

import tf.transformations as tft
import tfx

import code

# adapted and adjusted from Greg Kahn's code (ImageProcessing.py)

########################################
#             CONSTANTS                #
########################################

TRANSLATION_R2L = (-0.05137, 0, 0.00136) #FIXME stick in translation and rotation matrices
ROTATION_R2L = (0, -0.108380311, 0)
ORANGE_LOWER = cv.Scalar( 6, 130, 130)
ORANGE_UPPER = cv.Scalar( 10,255, 255)
BLUE_LOWER = cv.Scalar(105,120, 120)
BLUE_UPPER = cv.Scalar(115,255, 255)


########################################
#    IMAGE-RELATED HELPER METHODS      #
########################################

""" thresholds an image for a certain range of hsv values """ #FIXME: does eroding and dilating cause a big performance hit?
def threshold(image, hsvImg, threshImg, lowerHSV, upperHSV):
    cv.Smooth(image, image, cv.CV_GAUSSIAN, 3, 0)
    cv.CvtColor(image, hsvImg, cv.CV_BGR2HSV) 
    cv.InRangeS(hsvImg, lowerHSV, upperHSV, threshImg)
    #cv.Erode(threshImg, threshImg, None, 1)
    #cv.Dilate(threshImg, threshImg, None, 1)
    #cv.Erode(threshImg, threshImg, None, 1)
    #cv.Dilate(threshImg, threshImg, None, 1)
    #cv.Erode(threshImg, threshImg, None, 1)
    return threshImg

########################################
#     3D GEOMETRY HELPER METHODS       #
########################################

""" takes in two lines in (x1, y1, x2, y2) form and finds the line on top """
def find_top_line(line1, line2):
    if line2[3] > line1[3]:
	return line2
    else:
	return line1

""" finds lines in a binary image using hough line transform, after running canny edge detection """	
def find_lines(image):    
    edges = cv2.Canny(np.asarray(image[:,:]), 80, 120)
    lines = cv2.HoughLinesP(edges, 1, math.pi/180, 1, None, 10, 1)
    return (lines, cv.fromarray(edges))

""" finds the distance between two points """
def distance(x1, y1, x2, y2):
    return ((x2-x1)**2+(y2-y1)**2)**0.5

""" given a list of lines in (x1, y1, x2, y2) form, finds the longest line """
def get_max_line(lines): 
    lengths = []
    for line in lines:
	lengths.append(distance(line[0], line[1], line[2], line[3]))
    max_length = max(lengths)
    i = lengths.index(max_length)
    return lines[i]

""" takes in a list of lines in (x1, y1, x2, y2) form and returns a list of the n longest lines """
def get_n_lines(lines, n):
    lengths_to_lines = {}
    for line in lines:
	lengths_to_lines[distance(line[0], line[1], line[2], line[3])] = line
    lengths = list(lengths_to_lines.keys())
    lengths.sort()
    result = []
    if len(lengths) >= n:
	k = n
    else:
	k = len(lengths)
    for i in range(0, k):
	result.append(lengths_to_lines[lengths[i]])
    return result

""" takes in translation and rotation vectors and a plane, and transforms the plane accordingly """
def transform_plane(translation, rotation, plane):
    result_plane = (rotate_vector(rotation, plane[0]), (plane[1][0]+translation[0], plane[1][1]+translation[1], plane[1][2]+translation[2])) #FIXME get the translation and rotation working properly
    return result_plane

""" where a plane is the tuple (normal, point); returns unit vector corresponding to the normal of the intersection of two planes """
def intersect_planes(plane1, plane2):    
    cross_product = np.cross(plane1[0], plane2[0])
    return cross_product
	
""" helper function to rotate a vector given yaw-pitch-roll values """ #FIXME looks like a place where an error might be
def rotate_vector(yaw_pitch_roll, vector):
    a = yaw_pitch_roll[0]
    b = yaw_pitch_roll[1]
    c = yaw_pitch_roll[2]
    rot_matrix = np.array([[math.cos(a)*math.cos(b), math.cos(a)*math.sin(b)*math.sin(c)-math.sin(a)*math.cos(c), math.cos(a)*math.sin(b)*math.cos(c)+math.sin(a)*math.sin(c)], [math.sin(a)*math.cos(b), math.sin(a)*math.sin(b)*math.sin(c)+math.cos(a)*math.cos(c), math.sin(a)*math.sin(b)*math.cos(c)-math.cos(a)*math.sin(c)], [-math.sin(b), math.cos(b)*math.sin(c), math.cos(b)*math.cos(c)]])
    rot_matrix.reshape((3,3))
    result = rot_matrix*vector
    return result




##################
#   MAIN CLASS   #
##################

class ColorSegmenter():
    def __init__(self, left_camera, right_camera):
        
        self.leftInfo = self.rightInfo = None

        self.foundColorLeft = False
        self.foundColorRight = False
	
        self.listener = tf.TransformListener()
	self.bridge = cv_bridge.CvBridge()

        rospy.Subscriber('/stereo/%s/image_rect_color'%left_camera, Image, self.leftImageCallback)
        rospy.Subscriber('/stereo/%s/image_rect_color'%right_camera, Image, self.rightImageCallback)
        rospy.Subscriber('/stereo/%s/camera_info'%left_camera, CameraInfo, self.leftInfoCallback)
        rospy.Subscriber('/stereo/%s/camera_info'%right_camera, CameraInfo, self.rightInfoCallback)

	self.blue_right_lines = []
	self.orange_right_lines = []
	self.blue_left_lines = []
	self.orange_left_lines = []
	self.left_orange_found = False
	self.left_blue_found = False
	self.right_orange_found = False
	self.right_blue_found = False
	

    def convertStereo(self, u, v, disparity):
        """
        Converts two pixel coordinates u and v along with the disparity to give PointStamped       
        """
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
        (x,y,z) = stereoModel.projectPixelTo3d((u,v), disparity)
        
        cameraPoint = PointStamped()
        cameraPoint.header.frame_id = self.leftInfo.header.frame_id
        cameraPoint.header.stamp = rospy.Time.now()
        cameraPoint.point = Point(x,y,z)

        #self.listener.waitForTransform(self.outputFrame, cameraPoint.header.frame_id, rospy.Time.now(), rospy.Duration(4.0))
        #outputPoint = self.listener.transformPoint(self.outputFrame, cameraPoint)
        return cameraPoint
    
    ################################
    #   SUBSCRIBER BOUND METHODS   #
    ################################
    def leftInfoCallback(self, info):
	""" saves the info for the left camera	"""
        self.leftInfo = info

    def rightInfoCallback(self, info):
	""" saves the info for the right camera	"""
        self.rightInfo = info

    def leftImageCallback(self, image):
	""" """
	#left_edges, self.foundColorLeft, self.left_lines = self.process(image, "left")
	self.blue_left_lines, self.blue_left_edges, self.left_blue_found,self.orange_left_lines, self.orange_left_edges,self.left_orange_found = self.process(image, "left")
	self.handleBoth()
        #cv.ShowImage('Left Thresholded', left_thresh)
	cv.ShowImage('Left Blue Edges', self.blue_left_edges)
	cv.ShowImage('Left Orange Edges', self.orange_left_edges)
        cv.WaitKey(3)

    def rightImageCallback(self, image):
	""" """
	self.blue_right_lines, self.blue_right_edges,self.right_blue_found, self.orange_right_lines, self.orange_right_edges ,self.right_orange_found = self.process(image, "right")
	self.handleBoth()
        #cv.ShowImage('Right Viewer', right_thresh)
	#cv.ShowImage('Right Edges', right_edges)
        #cv.WaitKey(3)

    def handleBoth(self):
	""" returns the unit vector corresponding to the orientation of the colored tape """
	if self.left_orange_found and self.left_blue_found and self.right_orange_found and self.right_blue_found:
	    blue_vector = self.find_unit_vector(self.blue_left_lines[0], self.blue_right_lines[0], "blue")
	    orange_vector = self.find_unit_vector(self.orange_left_lines[0], self.orange_right_lines[0], "orange")
	    quat = self.get_orientation_from_lines(blue_vector, orange_vector)
	    print tfx.tb_angles(quat)

    ##############################
    #       HELPER METHODS       #
    ##############################
    def find_unit_vector(self, left_lines, right_lines, color):
        longest_left = get_max_line(left_lines)
        longest_right = get_max_line(right_lines)
        left_plane = self.transform_to_world(longest_left, "left") 
        right_plane = self.transform_to_world(longest_right, "right")
        transformed_plane = transform_plane(TRANSLATION_R2L, ROTATION_R2L, right_plane) 
        unit_vector = intersect_planes(left_plane, right_plane)
        return unit_vector      
	    
    def process(self, image, flag):
	"""
	thresholds the image for a certain hsv range and returns the coordinates of the centroid, 
	and the coordinates of the closest point to the centroid
	"""
	cv_image = self.bridge.imgmsg_to_cv(image, "bgr8")
	orangeThreshImg = cv.CreateImage((1280,960),8,1)
	orangeThreshImg = threshold(cv_image, hsvImg, orangeThreshImg, ORANGE_LOWER, ORANGE_UPPER)
	(orange_lines, orange_edges) = find_lines(orangeThreshImg)
	blueThreshImg = cv.CreateImage((1280,960),8,1)
	blueThreshImg = threshold(cv_image, hsvImg, blueThreshImg, BLUE_LOWER, BLUE_UPPER)
	(blue_lines, blue_edges) = find_lines(blueThreshImg)
	blue_found = (blue_lines!=None)
	orange_found = (orange_lines!=None)
	return (blue_lines, blue_edges, blue_found, orange_lines, orange_edges, orange_found)

    def transform_to_world(self, line, flag):
	""" takes in a pixel line as defined by two endpoints and transforms it to real-world coordinates based on camera info"""
	if flag == "right":
	    info = self.rightInfo
	elif flag == "left":
	    info = self.leftInfo
	x1 = line[0]
	y1 = line[1]
	x2 = line[2]
	y2 = line[3]
	pinhole_model = image_geometry.PinholeCameraModel()
	pinhole_model.fromCameraInfo(info)	
	ray1 = pinhole_model.projectPixelTo3dRay((x1, y1))
	ray2 = pinhole_model.projectPixelTo3dRay((x2, y2))
	plane_normal = np.cross(ray1, ray2) 
	return (plane_normal, (0,0,0))

    def get_orientation_from_lines(self, v0, v1):
        """
        v0 and v1 are vectors representing two lines
        that are KNOWN to be in the same plane
        """
        # get into an np array
        v0, v1 = np.array(v0), np.array(v1)
        # normalize
        v0 = v0 / np.linalg.norm(v0)
        v1 = v1 / np.linalg.norm(v1)
        # find normal
        n = np.cross(v0, v1)
        # stack into a matrix
        rotMat = np.vstack((v0, v1, n)).T
        # find the quaternion xyzw
        tbRot = tfx.tb_angles(rotMat)
        quat = tbRot.quaternion
        #code.interact(local=locals())
        return list(quat)

##############################
#      EXECUTION CODE        #
##############################
def test_orientation():
    rospy.init_node('orientation_from_lines',anonymous=True)
    gs = ColorSegmenter('left','right')
    
    v0 = [1, 0, 0]
    v1 = [0, 1, 0]
    
    gs.get_orientation_from_lines(v0, v1)

def test_everything():
    rospy.init_node('color_segmenter')
    left_camera = 'left'
    right_camera = 'right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()
    gs.foundColorLeft = True
    gs.foundColorRight = True
    gs.left_lines = [[(0, 0, 15, 15)]]
    gs.right_lines = [[(0, 0, 20, 20)]]
    vector = gs.handleBoth()
    print vector

def main():
    rospy.init_node('color_segmenter')
    left_camera = 'left'
    right_camera = 'right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()


if __name__ == '__main__':
    #test_everything()
    main()
    #test_orientation()
