#!/usr/bin/env python

# Import required Python code.
import roslib
roslib.load_manifest('raven_pose_estimator')
import rospy

import cv
import cv2
import numpy as np
from scipy import linalg
import math
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import cv_bridge
from geometry_msgs.msg import Point, PointStamped
import tf
import image_geometry
import time

import message_filters
from threading import Lock

import tf.transformations as tft
import tfx

import code

# adapted and adjusted from Greg Kahn's code (ImageProcessing.py)

########################################
#             CONSTANTS                #
########################################

TRANSLATION_R2L = (-0.05137, 0, 0.00136) #FIXME stick in translation and rotation matrices
ROTATION_R2L = (0, -0.108380311, 0)
ORANGE_LOWER = cv.Scalar( 6, 130, 130)
ORANGE_UPPER = cv.Scalar( 10,255, 255)
BLUE_LOWER = cv.Scalar(105,120, 120)
BLUE_UPPER = cv.Scalar(115,255, 255)


########################################
#    IMAGE-RELATED HELPER METHODS      #
########################################

""" thresholds an image for a certain range of hsv values """ #FIXME: does eroding and dilating cause a big performance hit?
def threshold(image, hsvImg, threshImg, lowerHSV, upperHSV):
    cv.Smooth(image, image, cv.CV_GAUSSIAN, 3, 0)
    cv.CvtColor(image, hsvImg, cv.CV_BGR2HSV) 
    cv.InRangeS(hsvImg, lowerHSV, upperHSV, threshImg)
    #cv.Erode(threshImg, threshImg, None, 1)
    #cv.Dilate(threshImg, threshImg, None, 1)
    #cv.Erode(threshImg, threshImg, None, 1)
    #cv.Dilate(threshImg, threshImg, None, 1)
    #cv.Erode(threshImg, threshImg, None, 1)
    return threshImg

########################################
#     3D GEOMETRY HELPER METHODS       #
########################################

def flip_origin(point, height):
    x = point[0]
    y = point[1]
    result_x = x
    result_y = height-y
    return (result_x, result_y)

def flip_line(line, height):
    (x1, y1) = flip_origin((line[0], line[1]), 1280)
    (x2, y2) = flip_origin((line[2], line[3]), 1280)
    return (x1, y1, x2, y2)

def angleBetweenQuaternions(quat0, quat1):
    """
    Returns angle between quat0 and quat1 in degrees
    """
    q0 = np.array(quat0)
    q1 = np.array(quat1)
    theta = math.acos(2*np.dot(q0,q1)**2 - 1)
    theta = theta*(180.0/math.pi)
    return theta

""" takes in two lines in (x1, y1, x2, y2) form and finds the line on top """
def find_top_line(line1, line2):
    if line2[3] > line1[3]:
	return line2
    else:
	return line1

def invert_vector(v):
    result = [-t for t in v]
    return result

""" finds lines in a binary image using hough line transform, after running canny edge detection """	
def find_lines(image):    
    edges = cv2.Canny(np.asarray(image[:,:]), 80, 120)
    lines = cv2.HoughLinesP(edges, 1, math.pi/180, 1, None, 20, 1)
    flipped_lines = []
    if lines != None:
	for line in lines[0]:
	    flipped_lines.append(flip_line(line, 1280))
    return (lines, cv.fromarray(edges))

""" finds the distance between two points """
def distance(x1, y1, x2, y2):
    return ((x2-x1)**2+(y2-y1)**2)**0.5

""" given a list of lines in (x1, y1, x2, y2) form, finds the longest line """
def get_max_line(lines): 
    lengths = []
    for line in lines:
	lengths.append(distance(line[0], line[1], line[2], line[3]))
    max_length = max(lengths)
    i = lengths.index(max_length)
    return lines[i]

""" takes in a list of lines in (x1, y1, x2, y2) form and returns a list of the n longest lines """
def get_n_lines(lines, n):
    lengths_to_lines = {}
    for line in lines:
	lengths_to_lines[distance(line[0], line[1], line[2], line[3])] = line
    lengths = list(lengths_to_lines.keys())
    lengths.sort()
    result = []
    if len(lengths) >= n:
	k = n
    else:
	k = len(lengths)
    for i in range(0, k):
	result.append(lengths_to_lines[lengths[i]])
    return result

""" takes in translation and rotation vectors and a plane, and transforms the plane accordingly """
def transform_plane(translation, rotation, plane):
    result_plane = (rotate_vector(rotation, plane[0]), (plane[1][0]+translation[0], plane[1][1]+translation[1], plane[1][2]+translation[2])) #FIXME get the translation and rotation working properly
    return result_plane

""" where a plane is the tuple (normal, point); returns unit vector corresponding to the normal of the intersection of two planes """
def intersect_planes(plane1, plane2):    
    cross_product = np.cross(plane1[0], plane2[0])
    #cross_product = np.cross(plane2[0], plane1[0])
    return cross_product
	
""" helper function to rotate a vector given yaw-pitch-roll values """ #FIXME looks like a place where an error might be
def rotate_vector(yaw_pitch_roll, vector):
    a = yaw_pitch_roll[0]
    b = yaw_pitch_roll[1]
    c = yaw_pitch_roll[2]
    rot_matrix = np.array([[math.cos(a)*math.cos(b), math.cos(a)*math.sin(b)*math.sin(c)-math.sin(a)*math.cos(c), math.cos(a)*math.sin(b)*math.cos(c)+math.sin(a)*math.sin(c)], [math.sin(a)*math.cos(b), math.sin(a)*math.sin(b)*math.sin(c)+math.cos(a)*math.cos(c), math.sin(a)*math.sin(b)*math.cos(c)-math.cos(a)*math.sin(c)], [-math.sin(b), math.cos(b)*math.sin(c), math.cos(b)*math.cos(c)]])
    rot_matrix.reshape((3,3))
    result = rot_matrix*vector
    return result




##################
#   MAIN CLASS   #
##################

class ColorSegmenter():
    def __init__(self, left_camera, right_camera):
        
        self.leftInfo = self.rightInfo = None

        self.foundColorLeft = False
        self.foundColorRight = False
	
        self.listener = tf.TransformListener()
	self.bridge = cv_bridge.CvBridge()

        rospy.Subscriber('/stereo/%s/image_rect_color'%left_camera, Image, self.leftImageCallback)
        rospy.Subscriber('/stereo/%s/image_rect_color'%right_camera, Image, self.rightImageCallback)
        rospy.Subscriber('/stereo/%s/camera_info'%left_camera, CameraInfo, self.leftInfoCallback)
        rospy.Subscriber('/stereo/%s/camera_info'%right_camera, CameraInfo, self.rightInfoCallback)
	self.blue_right_lines = []
	self.orange_right_lines = []
	self.blue_left_lines = []
	self.orange_left_lines = []
	self.left_orange_found = False
	self.left_blue_found = False
	self.right_orange_found = False
	self.right_blue_found = False
	self.prev_quat = None
	

    def convertStereo(self, u, v, disparity):
        """
        Converts two pixel coordinates u and v along with the disparity to give PointStamped       
        """
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
        (x,y,z) = stereoModel.projectPixelTo3d((u,v), disparity)
        
        cameraPoint = PointStamped()
        cameraPoint.header.frame_id = self.leftInfo.header.frame_id
        cameraPoint.header.stamp = rospy.Time.now()
        cameraPoint.point = Point(x,y,z)

        #self.listener.waitForTransform(self.outputFrame, cameraPoint.header.frame_id, rospy.Time.now(), rospy.Duration(4.0))
        #outputPoint = self.listener.transformPoint(self.outputFrame, cameraPoint)
        return cameraPoint
    
    ################################
    #   SUBSCRIBER BOUND METHODS   #
    ################################
    def leftInfoCallback(self, info):
	""" saves the info for the left camera	"""
        self.leftInfo = info

    def rightInfoCallback(self, info):
	""" saves the info for the right camera	"""
        self.rightInfo = info

    def leftImageCallback(self, image):
	""" """
	#left_edges, self.foundColorLeft, self.left_lines = self.process(image, "left")
	self.blue_left_lines, self.blue_left_edges, self.left_blue_found,self.orange_left_lines, self.orange_left_edges,self.left_orange_found = self.process(image, "left")
	self.handleBoth()
	#some debugging stuff
	"""if self.blue_left_lines != None:
	    image = self.bridge.imgmsg_to_cv(image, "bgr8")
	    blue_max_line = get_max_line(self.blue_left_lines[0])
	    print blue_max_line
	    np_version = np.asarray(image[:,:])
	    cv2.line(np_version, (blue_max_line[0], blue_max_line[1]), (blue_max_line[2], blue_max_line[3]), (0,0,255), 3)
	    image = cv.fromarray(np_version)"""
        #cv.ShowImage('Left Thresholded', left_thresh)
	#cv.ShowImage('Left Orange Edges', self.orange_left_edges)
	#cv.ShowImage('Left Blue Edges', self.blue_left_edges)
	#cv.ShowImage('lines', image)
        cv.WaitKey(3)

    def rightImageCallback(self, image):
	""" """
	self.blue_right_lines, self.blue_right_edges,self.right_blue_found, self.orange_right_lines, self.orange_right_edges ,self.right_orange_found = self.process(image, "right")
	self.handleBoth()
        #cv.ShowImage('Right Viewer', right_thresh)
	#cv.ShowImage('Right Edges', right_edges)
        #cv.WaitKey(3)

    def handleBoth(self):
	""" returns the unit vector corresponding to the orientation of the colored tape """
	if self.left_orange_found and self.left_blue_found and self.right_orange_found and self.right_blue_found:
	    blue_vector = self.find_unit_vector(self.blue_left_lines[0], self.blue_right_lines[0], "blue")
	    orange_vector = self.find_unit_vector(self.orange_left_lines[0], self.orange_right_lines[0], "orange")
	    #blue_vector = invert_vector(blue_vector)
	    #orange_vector = invert_vector(orange_vector)
	    quat = self.get_orientation_from_lines(blue_vector, orange_vector)
	    print "quaternion"
	    print quat
	    print "yaw pitch roll"
	    print tfx.tb_angles(quat)
	    if self.prev_quat != None:
		angle = angleBetweenQuaternions(self.prev_quat, quat) #FIXME uncomment this
		print "angle"
		print angle
	    self.prev_quat = quat

    ##############################
    #       HELPER METHODS       #
    ##############################
    def find_unit_vector(self, left_lines, right_lines, color):
        longest_left = get_max_line(left_lines)
        longest_right = get_max_line(right_lines)
        left_plane = self.transform_to_world(longest_left, "left") 
        right_plane = self.transform_to_world(longest_right, "right")
        transformed_plane = transform_plane(TRANSLATION_R2L, ROTATION_R2L, right_plane) 
        unit_vector = intersect_planes(left_plane, right_plane)
	#print "unit_vector"
	#print unit_vector
        return unit_vector      
	    
    def process(self, image, flag):
	"""
	thresholds the image for a certain hsv range and returns the coordinates of the centroid, 
	and the coordinates of the closest point to the centroid
	"""
	cv_image = self.bridge.imgmsg_to_cv(image, "bgr8")
	orangeThreshImg = cv.CreateImage((1280,960),8,1)
	hsvImg = cv.CreateImage((1280,960),8,3)
	orangeThreshImg = threshold(cv_image, hsvImg, orangeThreshImg, ORANGE_LOWER, ORANGE_UPPER)
	(orange_lines, orange_edges) = find_lines(orangeThreshImg)
	blueThreshImg = cv.CreateImage((1280,960),8,1)
	blueThreshImg = threshold(cv_image, hsvImg, blueThreshImg, BLUE_LOWER, BLUE_UPPER)
	(blue_lines, blue_edges) = find_lines(blueThreshImg)
	blue_found = (blue_lines!=None)
	orange_found = (orange_lines!=None)
	return (blue_lines, blue_edges, blue_found, orange_lines, orange_edges, orange_found)

    def transform_to_world(self, line, flag):
	""" takes in a pixel line as defined by two endpoints and transforms it to real-world coordinates based on camera info"""
	if flag == "right":
	    info = self.rightInfo
	elif flag == "left":
	    info = self.leftInfo
	x1 = line[0]
	y1 = line[1]
	x2 = line[2]
	y2 = line[3]
	#print "points"
	#print x1, y1
	#print x2, y2
	pinhole_model = image_geometry.PinholeCameraModel()
	pinhole_model.fromCameraInfo(info)	
	ray1 = pinhole_model.projectPixelTo3dRay((x1, y1))
	ray2 = pinhole_model.projectPixelTo3dRay((x2, y2))
	#print flag
	#print "rays"
	#print ray1
	#print ray2
	plane_normal = np.cross(ray1, ray2) 
	return (plane_normal, (0,0,0))

    def get_orientation_from_lines(self, v0, v1):
        """
        v0 and v1 are vectors representing two lines
        that are KNOWN to be in the same plane
        """
        # get into an np array
	#v1 = invert_vector(v1)
	#v0 = invert_vector(v0)
        v0, v1 = np.array(v0), np.array(v1)
	#print "DOT PRODUCT"
	#print v0
	#print v1
	#print np.dot(v0,v1)
        # normalize
        v0 = v0 / np.linalg.norm(v0) 
        v1 = v1 / np.linalg.norm(v1) 
        # find normal
        n = np.cross(v0, v1)
	#n = np.cross(v1, v0)
        # stack into a matrix
        #rotMat = np.vstack((n, v0, v1)).T
	rotMat = np.vstack((v0, v1, n)).T
	matrix = rotMat
	#matrix = linalg.orth(rotMat) #FIXME uncomment
	#print "matrices"
	#print rotMat
	#print orthogonalized
        # find the quaternion xyzw
        tbRot = tfx.tb_angles(matrix)
        quat = tbRot.quaternion
        #code.interact(local=locals())
        return list(quat)

##############################
#      EXECUTION CODE        #
##############################
def test_orientation():
    rospy.init_node('orientation_from_lines',anonymous=True)
    gs = ColorSegmenter('left','right')
    
    v0 = [1, 0, 0]
    v1 = [0, 1, 0]
    
    gs.get_orientation_from_lines(v0, v1)

def test_everything():
    rospy.init_node('color_segmenter')
    left_camera = 'left'
    right_camera = 'right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()
    gs.foundColorLeft = True
    gs.foundColorRight = True
    gs.left_lines = [[(0, 0, 15, 15)]]
    gs.right_lines = [[(0, 0, 20, 20)]]
    vector = gs.handleBoth()
    print vector

def main():
    rospy.init_node('color_segmenter')
    left_camera = 'left'
    right_camera = 'right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()


if __name__ == '__main__':
    #test_everything()
    main()
    #test_orientation()
