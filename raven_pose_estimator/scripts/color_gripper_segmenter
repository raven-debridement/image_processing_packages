#!/usr/bin/env python

# Import required Python code.
import roslib
roslib.load_manifest('raven_pose_estimator')
import rospy

import cv
import cv2
import numpy as np
import math
from std_msgs.msg import String
from sensor_msgs.msg import Image, CameraInfo
import cv_bridge
from geometry_msgs.msg import Point, PointStamped
import tf
import image_geometry
import time

import message_filters
from threading import Lock

import tf.transformations as tft
import tfx

import code

# adapted and adjusted from Greg Kahn's code (ImageProcessing.py)

class ColorSegmenter():
    def __init__(self, left_camera, right_camera):
        
        self.leftInfo = self.rightInfo = None

        self.foundColorLeft = False
        self.foundColorRight = False
	
        self.listener = tf.TransformListener()
	self.bridge = cv_bridge.CvBridge()
        self.outputFrame = 'base_link'
        rospy.Subscriber('%s/image_rect_color'%left_camera, Image, self.leftImageCallback)
        rospy.Subscriber('%s/image_rect_color'%right_camera, Image, self.rightImageCallback)
        rospy.Subscriber('%s/camera_info'%left_camera, CameraInfo, self.leftInfoCallback)
        rospy.Subscriber('%s/camera_info'%right_camera, CameraInfo, self.rightInfoCallback)
	self.pointpub = rospy.Publisher("foam_points", PointStamped)
	self.foundColorRight = False
	self.foundColorLeft = False
	self.xyCloseLeft = None
	self.xyCloseRight = None
	self.xyCentroidLeft = None
	self.xyCentroidRight = None	
	self.leftHsvImg = cv.CreateImage((1280,960), 8, 3)
	self.leftThreshImg = cv.CreateImage((1280,960), 8, 1)
	self.rightHsvImg = cv.CreateImage((1280,960), 8, 3)
	self.rightThreshImg = cv.CreateImage((1280,960), 8, 1)

    def convertStereo(self, u, v, disparity):
        """
        Converts two pixel coordinates u and v along with the disparity to give PointStamped       
        """
        stereoModel = image_geometry.StereoCameraModel()
        stereoModel.fromCameraInfo(self.leftInfo, self.rightInfo)
        (x,y,z) = stereoModel.projectPixelTo3d((u,v), disparity)
        
        cameraPoint = PointStamped()
        cameraPoint.header.frame_id = self.leftInfo.header.frame_id
        cameraPoint.header.stamp = rospy.Time.now()
        cameraPoint.point = Point(x,y,z)

        #self.listener.waitForTransform(self.outputFrame, cameraPoint.header.frame_id, rospy.Time.now(), rospy.Duration(4.0))
        #outputPoint = self.listener.transformPoint(self.outputFrame, cameraPoint)
        return cameraPoint
    
    def leftInfoCallback(self, info):
	""" saves the info for the left camera	"""
        self.leftInfo = info

    def rightInfoCallback(self, info):
	""" saves the info for the right camera	"""
        self.rightInfo = info

    def leftImageCallback(self, image):
	""" thresholds the image and finds the coordinates of the centroid, 
	as well as the closest pixel location to the centroid """
	left_edges, self.foundColorLeft, self.left_lines = self.process(image, "left")
	self.handleBoth()
        #cv.ShowImage('Left Thresholded', left_thresh)
	#cv.ShowImage('Left Edges', left_edges)
        #cv.WaitKey(3)

    def rightImageCallback(self, image):
	""" thresholds the image and finds the coordinates of the centroid, 
	as well as the closest pixel location to the centroid """
	right_edges, self.foundColorRight, self.right_lines = self.process(image, "right")
	self.handleBoth()
        #cv.ShowImage('Right Viewer', right_thresh)
	#cv.ShowImage('Right Edges', right_edges)
        #cv.WaitKey(3)

    def handleBoth(self):
	""" returns the unit vector corresponding to the orientation of the colored tape """
	if self.foundColorLeft and self.foundColorRight:
	    longest_left = self.get_max_line(self.left_lines)
	    longest_right = self.get_max_line(self.right_lines)
	    left_plane = self.transform_to_world(longest_left, "left")
	    right_plane = self.transform_to_world(longest_right, "right")
	    transformed_plane = self.transform_plane(translation_r2l, rotation_r2l, right_plane) #FIXME stick in translation and rotation matrices
	    unit_vector = self.intersect_planes(left_plane, right_plane)
	    
    def process(self, image, flag):
	"""
	thresholds the image for a certain hsv range and returns the coordinates of the centroid, 
	and the coordinates of the closest point to the centroid
	"""
	if flag=="right":
	    hsvImg = self.leftHsvImg
	    threshImg = self.leftThreshImg	
	elif flag=="left":
	    hsvImg = self.rightHsvImg	
	    threshImg = self.rightThreshImg
	cv_image = self.bridge.imgmsg_to_cv(image, "bgr8")	
	lowerHSV = cv.Scalar(100, 50, 140)
        upperHSV = cv.Scalar(120, 255, 255)
	threshImg = self.threshold(cv_image, hsvImg, threshImg, lowerHSV, upperHSV)
	(lines, edges) = self.find_lines(threshImg)
	if lines != None:
   	    max_line = self.get_max_line(lines[0])
	return (edges, found, lines)

    def threshold(self, image, hsvImg, threshImg, lowerHSV, upperHSV):
	""" thresholds an image for a certain range of hsv values """
	cv.Smooth(image, image, cv.CV_GAUSSIAN, 3, 0)
	cv.CvtColor(image, hsvImg, cv.CV_BGR2HSV) 
	cv.InRangeS(hsvImg, lowerHSV, upperHSV, threshImg)
	cv.Erode(threshImg, threshImg, None, 1)
        cv.Dilate(threshImg, threshImg, None, 1)
	cv.Erode(threshImg, threshImg, None, 1)
	cv.Dilate(threshImg, threshImg, None, 1)
	return threshImg

    def find_lines(self, image):
	""" finds lines in a binary image using hough line transform, after running canny edge detection"""	
	edges = cv2.Canny(np.asarray(image[:,:]), 80, 120)
	lines = cv2.HoughLinesP(edges, 1, math.pi/180, 1, None, 10, 1)
	return (lines, cv.fromarray(edges))

    def distance(self, x1, y1, x2, y2):
	return ((x2-x1)**2+(y2-y1)**2)**0.5

    def get_max_line(self, lines): #FIXME: we know want the top 4 lines, rather than just the longest line
	lengths = []
	for line in lines:
	    lengths.append(self.distance(line[0], line[1], line[2], line[3]))
	max_length = max(lengths)
	i = lengths.index(max_length)
	return lines[i]

    def transform_plane(self, translation, rotation, plane):
	""" takes in a transformation matrix and a plane """
	result_plane = (plane[0]*rotation, plane[1]+translation) #FIXME get the translation and rotation working properly
	return result_plane
	
    def transform_to_world(self, line, flag):
	""" takes in a pixel line as defined by two endpoints and transforms it to real-world coordinates based on camera info"""
	if flag == "right":
	    info = self.rightInfo
	elif flag == "left":
	    info = self.leftInfo
	x1 = line[0]
	y1 = line[1]
	x2 = line[2]
	y2 = line[3]
	pinhole_model = image_geometry.PinholeCameraModel()
	pinhole_model.fromcameraInfo(info)	
	ray1 = pinhole_model.projectPixelTo3dRay(x1, y1)
	ray2 = pinhole_model.projectPixelTo3dRay(x2, y2)
	plane_normal = np.cross(ray1, ray2) #FIXME not sure what the output type of projectPixelTo3dRay is
	return (plane_normal, (0,0,0))
	
    def intersect_planes(self, plane1, plane2):
	""" where a plane is the tuple (normal, point); returns unit vector corresponding to the intersection """
	cross_product = np.cross(plane1[0], plane2[0])
	return 

    #considered running skeletonization on the image; but this would be a big performance hit
    """def skeletonize(self, image):
	element = cv2.getStructuringElement(cv2.MORPH_CROSS, (3,3))
	done = False
	while not done:
	    cv.Erode(image, eroded, element)
	    cv.Dilate(eroded, temp, element)
	    cv.Subtract(image, temp, temp)
	    cv.Bitwise_Or("""

    def get_orientation_from_lines(self, v0, v1):
        """
        v0 and v1 are vectors representing two lines
        that are KNOWN to be in the same plane
        """
        # get into an np array
        v0, v1 = np.array(v0), np.array(v1)

        # normalize
        v0 = v0 / np.linalg.norm(v0)
        v1 = v1 / np.linalg.norm(v1)

        # find normal
        n = np.cross(v0, v1)

        # stack into a matrix
        rotMat = np.vstack((v0, v1, n)).T

        # find the quaternion xyzw
        tbRot = tfx.tb_angles(rotMat)
        quat = tbRot.quaternion

        #code.interact(local=locals())

        return list(quat)

def test_orientation():
    rospy.init_node('orientation_from_lines',anonymous=True)
    gs = ColorSegmenter('left','right')
    
    v0 = [1, 0, 0]
    v1 = [0, 1, 0]
    
    gs.get_orientation_from_lines(v0, v1)


def main():
    rospy.init_node('color_segmenter')
    left_camera = 'left'
    right_camera = 'right'
    gs = ColorSegmenter(left_camera, right_camera)
    rospy.spin()


if __name__ == '__main__':
    #main()
    test_orientation()
